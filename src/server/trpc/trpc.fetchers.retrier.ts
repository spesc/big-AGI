import { TRPCFetcherError } from '~/server/trpc/trpc.router.fetchers';
import { abortableDelay } from '~/server/wire';


const AIX_DEBUG_SERVER_RETRY = true;

const RETRY_PROFILES = {
  // network/DNS failures (never connected) -> fast retry
  network: {
    baseDelayMs: 500,
    maxDelayMs: 8000,
    jitterFactor: 0.25,
    maxAttempts: 3,      // 3 attempts total: immediate, then retry at ~0.5s, ~1s
  },
  // server overload (connected, but server busy) -> slower retry
  server: {
    baseDelayMs: 1000,
    maxDelayMs: 10000,
    jitterFactor: 0.5,    // 50% randomization
    maxAttempts: 4,      // 4 attempts total: immediate, then retry at ~1s, ~2s, ~4s
  },
} as const;

type RetryProfile = typeof RETRY_PROFILES[keyof typeof RETRY_PROFILES];


/**
 * Determines if a dispatch error is retryable and which profile to use.
 */
function selectRetryProfile(error: TRPCFetcherError | unknown): RetryProfile | null {
  if (!(error instanceof TRPCFetcherError))
    return null;

  if (error.category === 'connection')
    return RETRY_PROFILES.network; // DNS, TCP, timeouts, ... doesn't connect

  if (error.category === 'http' && error.httpStatus) {
    // 429 Too Many Requests: distinguish quota errors (don't retry) from rate limits (retry)
    if (error.httpStatus === 429) {
      const isQuotaError = /quota|billing/i.test(error.message);
      if (isQuotaError) {
        if (AIX_DEBUG_SERVER_RETRY)
          console.log(`[fetchers.retrier] Detected quota/billing error - will not retry`);
        return null; // Don't retry quota/billing errors - user needs to upgrade plan
      }
      return RETRY_PROFILES.server; // Retry temporary rate limits
    }

    // retriable server errors
    const retryCodes = [
      503, // Service Unavailable <- main one to retry
      502, // Bad Gateway
    ];
    if (retryCodes.includes(error.httpStatus))
      return RETRY_PROFILES.server;
  }

  return null;
}


/**
 * Creates a retryable promise that attempts the operation with exponential backoff.
 *
 * This returns a single promise that internally handles all retry attempts,
 * allowing it to be used with the existing heartbeatsWhileAwaiting pattern.
 *
 * Features:
 * - Exponential backoff: delay * 2^(attempt-1), capped at maxDelay
 * - Jitter: ±25% for network errors, ±50% for server errors (prevents thundering herd)
 * - Heartbeat safe: All delays capped at 10s to prevent connection timeouts
 *
 * @param operationFn The operation to retry (must be repeatable/idempotent)
 * @param abortSignal Signal to cancel retries
 * @returns Promise that resolves with the successful result or rejects with the final error
 */
export function createRetryablePromise<T>(operationFn: () => Promise<T>, abortSignal: AbortSignal): Promise<T> {
  return new Promise<T>(async (resolve, reject) => {
    let attemptNumber = 1;

    while (true) {
      try {

        // normal attempt, expecting success and setting the promise value
        const result = await operationFn();
        if (AIX_DEBUG_SERVER_RETRY && attemptNumber > 1) {
          // NOTE: console.warn to overwrite the lower level [POST/GET] warning logs
          console.warn(`[fetchers.retrier] ✅ Success after ${attemptNumber} attempts`);
        }
        resolve(result);
        return;

      } catch (error: any) {

        // aborted: forward the error immediately
        if (abortSignal.aborted) {
          if (AIX_DEBUG_SERVER_RETRY)
            console.log(`[fetchers.retrier] ⛔ User aborted at attempt ${attemptNumber}`);
          reject(error);
          return;
        }

        // check if error is retryable
        const rp = selectRetryProfile(error);

        // not retryable
        if (!rp) {
          if (AIX_DEBUG_SERVER_RETRY) {
            const errorInfo = !(error instanceof TRPCFetcherError) ? '' : `(${error.category}${error.httpStatus ? `, HTTP ${error.httpStatus}` : ''})`;
            console.log(`[fetchers.retrier] ❌ Not retryable ${errorInfo}}`); // removing the duplicate error message
            // console.log(`[fetchers.retrier] ❌ Not retryable ${errorInfo}: ${error?.message || error}`);
          }
          reject(error);
          return;
        }

        // exhausted attempts
        if (attemptNumber >= rp.maxAttempts) {
          if (AIX_DEBUG_SERVER_RETRY) {
            const errorInfo = !(error instanceof TRPCFetcherError) ? '' : `(${error.category}${error.httpStatus ? `, HTTP ${error.httpStatus}` : ''})`;
            console.warn(`[fetchers.retrier] ⚠️ All ${rp.maxAttempts - 1} retry attempts exhausted ${errorInfo}`);
          }
          reject(error);
          return;
        }

        // log retry decision with error details
        if (AIX_DEBUG_SERVER_RETRY) {
          const errorInfo = error instanceof TRPCFetcherError
            ? `(${error.category}${error.httpStatus ? `, HTTP ${error.httpStatus}` : ''})`
            : '';
          const profileType = rp === RETRY_PROFILES.network ? 'network' : 'server';
          console.log(`[fetchers.retrier] 🔄 Retryable error ${errorInfo} - using '${profileType}' profile`);
          // console.log(`[fetchers.retrier] 🔄 Retryable error ${errorInfo} - using ${profileType} profile: ${error?.message || error}`);
        }

        // calculate exponential backoff with jitter
        const exponentialDelay = rp.baseDelayMs * Math.pow(2, attemptNumber - 1);
        let delayMs = Math.min(exponentialDelay, rp.maxDelayMs);

        // add jitter to prevent thundering herd
        if (rp.jitterFactor > 0) {
          const jitterRange = delayMs * rp.jitterFactor;
          const randomJitter = (Math.random() * 2 - 1) * jitterRange; // ±jitterRange
          delayMs = Math.max(1, Math.round(delayMs + randomJitter));
        }

        attemptNumber++;
        if (AIX_DEBUG_SERVER_RETRY)
          console.log(`[fetchers.retrier] 🔄 Retrying attempt ${attemptNumber - 1}/${rp.maxAttempts - 1} after ${delayMs}ms delay`);

        // abortable wait
        if (await abortableDelay(delayMs, abortSignal)) {
          reject(error);
          return;
        }

        // -> loop continues for next attempt
      }
    }
  });
}